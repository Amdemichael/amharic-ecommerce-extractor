{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Lightweight Amharic NER Training\n",
       "\n",
       "This notebook is optimized for systems with limited resources (high CPU/Memory usage).\n",
       "It uses minimal memory and CPU to avoid kernel crashes."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 1: Setup and Memory Management\n",
       "import os\n",
       "import sys\n",
       "import gc\n",
       "import torch\n",
       "import warnings\n",
       "warnings.filterwarnings('ignore')\n",
       "\n",
       "# Force CPU and limit resources\n",
       "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
       "torch.set_num_threads(1)\n",
       "\n",
       "# Add src to path\n",
       "module_path = os.path.abspath(os.path.join('..'))\n",
       "if module_path not in sys.path:\n",
       "    sys.path.append(module_path)\n",
       "\n",
       "# Clear memory\n",
       "gc.collect()\n",
       "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
       "\n",
       "print(\"✅ Memory management setup complete\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 2: Import Lightweight Trainer\n",
       "try:\n",
       "    from src.lightweight_trainer import LightweightNERTrainer, LightweightConfig\n",
       "    print(\"✅ Lightweight trainer imported successfully\")\n",
       "except ImportError as e:\n",
       "    print(f\"❌ Import error: {e}\")\n",
       "    print(\"Please ensure all dependencies are installed\")\n",
       "    \n",
       "# Clear memory again\n",
       "gc.collect()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 3: Check Data Availability\n",
       "conll_path = '../data/annotated/amharic_ner.conll'\n",
       "if os.path.exists(conll_path):\n",
       "    print(f\"✅ Found CONLL data: {conll_path}\")\n",
       "    \n",
       "    # Count lines to estimate size\n",
       "    with open(conll_path, 'r', encoding='utf-8') as f:\n",
       "        lines = f.readlines()\n",
       "        sentences = sum(1 for line in lines if line.strip() == '')\n",
       "        print(f\"   Estimated sentences: {sentences}\")\n",
       "else:\n",
       "    print(\"❌ CONLL data not found\")\n",
       "    print(\"Please ensure data/annotated/amharic_ner.conll exists\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 4: Initialize Lightweight Trainer\n",
       "print(\"\\n=== Initializing Lightweight Trainer ===\\n\")\n",
       "\n",
       "# Configure for minimal resources\n",
       "config = LightweightConfig(\n",
       "    model_name=\"distilbert-base-multilingual-cased\",  # Smaller model\n",
       "    batch_size=2,  # Very small batch\n",
       "    num_epochs=1,  # Single epoch\n",
       "    max_length=128  # Short sequences\n",
       ")\n",
       "\n",
       "trainer = LightweightNERTrainer(config)\n",
       "print(f\"✅ Trainer initialized with model: {config.model_name}\")\n",
       "print(f\"   Batch size: {config.batch_size}\")\n",
       "print(f\"   Epochs: {config.num_epochs}\")\n",
       "print(f\"   Max length: {config.max_length}\")\n",
       "\n",
       "# Clear memory\n",
       "gc.collect()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 5: Load Data (Limited Samples)\n",
       "print(\"\\n=== Loading Data (Limited Samples) ===\\n\")\n",
       "\n",
       "try:\n",
       "    # Load only 30 samples to save memory\n",
       "    dataset = trainer.load_conll_data(conll_path, max_samples=30)\n",
       "    print(f\"✅ Data loaded: {len(dataset['train'])} training, {len(dataset['validation'])} validation samples\")\n",
       "    \n",
       "    # Clear memory\n",
       "    gc.collect()\n",
       "    \n",
       "except Exception as e:\n",
       "    print(f\"❌ Error loading data: {e}\")\n",
       "    print(\"This might be due to memory constraints or missing data\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 6: Train Model (Lightweight)\n",
       "print(\"\\n=== Training Model (Lightweight) ===\\n\")\n",
       "\n",
       "try:\n",
       "    # Create output directory\n",
       "    output_dir = \"../models/lightweight_ner\"\n",
       "    os.makedirs(output_dir, exist_ok=True)\n",
       "    \n",
       "    print(\"Starting lightweight training...\")\n",
       "    print(\"This may take a few minutes but uses minimal resources\")\n",
       "    \n",
       "    # Train model\n",
       "    train_result = trainer.train(dataset, output_dir)\n",
       "    \n",
       "    print(f\"✅ Training completed! Loss: {train_result.training_loss:.4f}\")\n",
       "    \n",
       "    # Clear memory\n",
       "    gc.collect()\n",
       "    \n",
       "except Exception as e:\n",
       "    print(f\"❌ Training error: {e}\")\n",
       "    print(\"This might be due to insufficient memory or other resource constraints\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 7: Test Predictions\n",
       "print(\"\\n=== Testing Model Predictions ===\\n\")\n",
       "\n",
       "try:\n",
       "    # Test texts\n",
       "    test_texts = [\n",
       "        \"LCD Writing Tablet ዋጋ 550 ብር\",\n",
       "        \"ስልክ ዋጋ 2500 ETB\",\n",
       "        \"ኮምፒዩተር ዋጋ 15000 ብር\"\n",
       "    ]\n",
       "    \n",
       "    for i, text in enumerate(test_texts):\n",
       "        predictions = trainer.predict(text)\n",
       "        print(f\"Text {i+1}: {text}\")\n",
       "        print(\"Predictions:\")\n",
       "        for token, label in predictions:\n",
       "            if label != 'O':\n",
       "                print(f\"  {token} -> {label}\")\n",
       "        print()\n",
       "    \n",
       "    print(\"✅ Prediction testing completed!\")\n",
       "    \n",
       "except Exception as e:\n",
       "    print(f\"❌ Prediction error: {e}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 8: Generate Simple Analytics\n",
       "print(\"\\n=== Generating Simple Analytics ===\\n\")\n",
       "\n",
       "try:\n",
       "    import pandas as pd\n",
       "    \n",
       "    # Load processed data if available\n",
       "    data_path = '../data/processed/telegram_processed.csv'\n",
       "    if os.path.exists(data_path):\n",
       "        # Load only first 100 rows to save memory\n",
       "        df = pd.read_csv(data_path, nrows=100)\n",
       "        \n",
       "        print(\"Simple Vendor Analytics:\")\n",
       "        print(f\"  Total messages: {len(df)}\")\n",
       "        print(f\"  Unique channels: {df['channel'].nunique()}\")\n",
       "        print(f\"  Average views: {df['views'].mean():.2f}\")\n",
       "        \n",
       "        # Simple lending score calculation\n",
       "        avg_views = df['views'].mean()\n",
       "        lending_score = min(avg_views / 1000 * 100, 100)  # Simple scoring\n",
       "        \n",
       "        print(f\"  Estimated lending score: {lending_score:.2f}%\")\n",
       "        \n",
       "    else:\n",
       "        print(\"⚠️  Processed data not found, skipping analytics\")\n",
       "    \n",
       "except Exception as e:\n",
       "    print(f\"❌ Analytics error: {e}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 9: Summary and Cleanup\n",
       "print(\"\\n=== Summary ===\\n\")\n",
       "\n",
       "print(\"✅ Lightweight NER Training Completed!\")\n",
       "print(\"\\nGenerated files:\")\n",
       "models_dir = \"../models\"\n",
       "if os.path.exists(models_dir):\n",
       "    for item in os.listdir(models_dir):\n",
       "        print(f\"  - {item}\")\n",
       "\n",
       "print(\"\\nKey Features:\")\n",
       "print(\"  - Minimal memory usage\")\n",
       "print(\"  - CPU-only training\")\n",
       "print(\"  - Small batch sizes\")\n",
       "print(\"  - Limited data samples\")\n",
       "print(\"  - Single epoch training\")\n",
       "\n",
       "print(\"\\nNext Steps:\")\n",
       "print(\"  1. If this worked, you can gradually increase batch_size and num_epochs\")\n",
       "print(\"  2. For better performance, consider using a machine with more RAM\")\n",
       "print(\"  3. The trained model is saved in ../models/lightweight_ner/\")\n",
       "\n",
       "# Final memory cleanup\n",
       "gc.collect()\n",
       "print(\"\\n✅ Memory cleanup completed\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 